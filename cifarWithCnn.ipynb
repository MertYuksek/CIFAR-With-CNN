{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logistic_regression.ipynb adlı not defterinin kopyası",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MertYuksek/CIFAR-With-CNN/blob/main/cifarWithCnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYazM-tQBWM8",
        "outputId": "7605b27a-35b2-4a38-c2c2-fe7df6ea6ae7"
      },
      "source": [
        "%reset\n",
        "# import libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "import copy\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "# model summary info\n",
        "from torchsummary import summary\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "display.set_matplotlib_formats('svg')\n",
        "\n",
        "import torchvision.transforms as T\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader,Subset"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wAUDDKcB5Z9"
      },
      "source": [
        "# use GPU if available\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqZkUyLkBj4_",
        "outputId": "727dd6f1-1020-4872-c941-c878aa846ef6"
      },
      "source": [
        "transform  = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
        "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "devset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "print(trainset.data.shape) # in loader shape is changed\n",
        "print(devset.data.shape)\n",
        "batchsize    = 32\n",
        "train_loader = DataLoader(trainset,batch_size=batchsize,shuffle=True,drop_last=True)\n",
        "dev_loader = DataLoader(devset,batch_size=batchsize,shuffle=True,drop_last=True)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQLTSEcBBnjs"
      },
      "source": [
        "# create a class for the model\n",
        "def makeTheNet(printtoggle=False):\n",
        "\n",
        "  class cifarnet(nn.Module):\n",
        "    def __init__(self,printtoggle):\n",
        "      super().__init__()\n",
        "      \n",
        "      # print toggle\n",
        "      self.print = printtoggle\n",
        "\n",
        "      ### -------------- feature map layers -------------- ###\n",
        "      # first convolution layer\n",
        "      self.conv1  = nn.Conv2d(3,64,3,padding=1)\n",
        "      self.bnorm1 = nn.BatchNorm2d(64) # input the number of channels in this layer\n",
        "      # output size: (32+2*1-3)/1 + 1 = 32/2 = 16 (/2 b/c maxpool)\n",
        "\n",
        "      # second convolution layer\n",
        "      self.conv2  = nn.Conv2d(64,128,3,padding=0)\n",
        "      self.bnorm2 = nn.BatchNorm2d(128) # input the number of channels in this layer\n",
        "      # output size: (16+2*0-3)/1 + 1 = 14/2 = 7 (/2 b/c maxpool)\n",
        "\n",
        "      # Third convolution layer\n",
        "      self.conv3  = nn.Conv2d(128,256,3,padding=0)\n",
        "      self.bnorm3 = nn.BatchNorm2d(256) # input the number of channels in this layer\n",
        "      # output size: (7+2*0-3)/1 + 1 = 5/2 = 2 (/2 b/c maxpool)\n",
        "\n",
        "      ### -------------- linear decision layers -------------- ###\n",
        "      self.fc1 = nn.Linear(2*2*256,256)\n",
        "      self.bn1 = nn.BatchNorm1d(num_features=256)\n",
        "      self.fc1_2 = nn.Linear(256,64)\n",
        "      self.bn2 = nn.BatchNorm1d(num_features=64)\n",
        "      self.fc2 = nn.Linear(64,10)\n",
        "\n",
        "    def forward(self,x):\n",
        "      \n",
        "      if self.print: print(f'Input: {list(x.shape)}')\n",
        "      \n",
        "      # first block: convolution -> maxpool -> batchnorm -> relu\n",
        "      x = F.max_pool2d(self.conv1(x),2)\n",
        "      x = F.leaky_relu(self.bnorm1(x))\n",
        "      if self.print: print(f'First CPR block: {list(x.shape)}')\n",
        "\n",
        "      # second block: convolution -> maxpool -> batchnorm -> relu\n",
        "      x = F.max_pool2d(self.conv2(x),2)\n",
        "      x = F.leaky_relu(self.bnorm2(x))\n",
        "      if self.print: print(f'Second CPR block: {list(x.shape)}')\n",
        "\n",
        "      # third block: convolution -> maxpool -> batchnorm -> relu\n",
        "      x = F.max_pool2d(self.conv3(x),2)\n",
        "      x = F.leaky_relu(self.bnorm3(x))\n",
        "      if self.print: print(f'Second CPR block: {list(x.shape)}')\n",
        "\n",
        "      # reshape for linear layer\n",
        "      nUnits = x.shape.numel()/x.shape[0]\n",
        "      x = x.view(-1,int(nUnits))\n",
        "      if self.print: print(f'Vectorized: {list(x.shape)}')\n",
        "      \n",
        "      # linear layers\n",
        "      x = F.leaky_relu(self.fc1(x))\n",
        "      x = self.bn1(x)\n",
        "      x = F.dropout(x, p=0.5, training=self.training)\n",
        "      x = F.leaky_relu(self.fc1_2(x))\n",
        "      x = self.bn2(x)\n",
        "      x = F.dropout(x, p=0.5, training=self.training)\n",
        "      x = self.fc2(x)\n",
        "      if self.print: print(f'Final output: {list(x.shape)}')\n",
        "\n",
        "      return x\n",
        "\n",
        "  # create the model instance\n",
        "  net = cifarnet(printtoggle)\n",
        "  \n",
        "  # loss function\n",
        "  lossfun = nn.CrossEntropyLoss()\n",
        "\n",
        "  # optimizer\n",
        "  optimizer = torch.optim.Adam(net.parameters(),lr=.001)\n",
        "\n",
        "  return net,lossfun,optimizer"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5zF93qfBpwA",
        "outputId": "46edd81b-b91e-42f0-e0dd-c07ac455d08b"
      },
      "source": [
        "# test the model with one batch\n",
        "net,lossfun,optimizer = makeTheNet(True)\n",
        "\n",
        "X,y = iter(train_loader).next()\n",
        "yHat = net(X)\n",
        "\n",
        "# check size of output\n",
        "print('\\nOutput size:')\n",
        "print(yHat.shape)\n",
        "\n",
        "# # now let's compute the loss\n",
        "loss = lossfun(yHat,torch.squeeze(y).long())\n",
        "print(' ')\n",
        "print('Loss:')\n",
        "print(loss)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [32, 3, 32, 32]\n",
            "First CPR block: [32, 64, 16, 16]\n",
            "Second CPR block: [32, 128, 7, 7]\n",
            "Second CPR block: [32, 256, 2, 2]\n",
            "Vectorized: [32, 1024]\n",
            "Final output: [32, 10]\n",
            "\n",
            "Output size:\n",
            "torch.Size([32, 10])\n",
            " \n",
            "Loss:\n",
            "tensor(2.6906, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHGCPSehBrwU",
        "outputId": "2a705109-7ef0-4b8d-9499-a86a0a7bc373"
      },
      "source": [
        "# count the total number of parameters in the model\n",
        "summary(net.to(device),(3,32,32))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [2, 3, 32, 32]\n",
            "First CPR block: [2, 64, 16, 16]\n",
            "Second CPR block: [2, 128, 7, 7]\n",
            "Second CPR block: [2, 256, 2, 2]\n",
            "Vectorized: [2, 1024]\n",
            "Final output: [2, 10]\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
            "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
            "            Conv2d-3          [-1, 128, 14, 14]          73,856\n",
            "       BatchNorm2d-4            [-1, 128, 7, 7]             256\n",
            "            Conv2d-5            [-1, 256, 5, 5]         295,168\n",
            "       BatchNorm2d-6            [-1, 256, 2, 2]             512\n",
            "            Linear-7                  [-1, 256]         262,400\n",
            "       BatchNorm1d-8                  [-1, 256]             512\n",
            "            Linear-9                   [-1, 64]          16,448\n",
            "      BatchNorm1d-10                   [-1, 64]             128\n",
            "           Linear-11                   [-1, 10]             650\n",
            "================================================================\n",
            "Total params: 651,850\n",
            "Trainable params: 651,850\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.93\n",
            "Params size (MB): 2.49\n",
            "Estimated Total Size (MB): 3.42\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BOg2PJMBsXz"
      },
      "source": [
        "# a function that trains the model\n",
        "\n",
        "def function2trainTheModel():\n",
        "\n",
        "  # number of epochs\n",
        "  numepochs = 10\n",
        "  \n",
        "  # create a new model\n",
        "  net,lossfun,optimizer = makeTheNet()\n",
        "\n",
        "  # send the model to the GPU\n",
        "  net.to(device)\n",
        "\n",
        "  # initialize losses\n",
        "  trainLoss = torch.zeros(numepochs)\n",
        "  devLoss  = torch.zeros(numepochs)\n",
        "  trainAccr  = torch.zeros(numepochs)\n",
        "  devAccr   = torch.zeros(numepochs)\n",
        "\n",
        "\n",
        "  # loop over epochs\n",
        "  for epochi in range(numepochs):\n",
        "    print(\"epoch : \"+str(epochi))\n",
        "    # loop over training data batches\n",
        "    net.train()\n",
        "    batchLoss = []\n",
        "    batchErr  = []\n",
        "    for X,y in train_loader:\n",
        "\n",
        "      # push data to GPU\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "\n",
        "      # forward pass and loss\n",
        "      yHat = net(X)\n",
        "      loss = lossfun(yHat,y.long())\n",
        "\n",
        "      # backprop\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # loss and error from this batch\n",
        "      batchLoss.append(loss.item())\n",
        "      batchErr.append( torch.mean((torch.argmax(yHat,axis=1) == y.long()).float()).item() )\n",
        "    # end of batch loop...\n",
        "\n",
        "    # and get average losses and error rates across the batches\n",
        "    trainLoss[epochi] = np.mean(batchLoss)\n",
        "    trainAccr[epochi]  = 100*np.mean(batchErr)\n",
        "\n",
        "    ### test performance\n",
        "    net.eval()\n",
        "    X,y = next(iter(dev_loader)) # extract X,y from test dataloader\n",
        "\n",
        "    # push data to GPU\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    with torch.no_grad(): # deactivates autograd\n",
        "      yHat = net(X)\n",
        "      loss = lossfun(yHat,y.long())\n",
        "    \n",
        "    #print(\"--------------------------------------------------------------\")\n",
        "    #t = torch.cuda.memory_stats(device)\n",
        "    #print(t)\n",
        "      \n",
        "    # get loss and error rate from the test batch\n",
        "    devLoss[epochi] = loss.item()\n",
        "    devAccr[epochi]  = 100*torch.mean((torch.argmax(yHat,axis=1) == y.long()).float()).item()\n",
        "\n",
        "    pl(trainLoss,devLoss,trainAccr,devAccr)\n",
        "  # end epochs\n",
        "\n",
        "  # function output\n",
        "  return trainLoss,devLoss,trainAccr,devAccr,net"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oU69yUCPByc9"
      },
      "source": [
        "def pl(trainLoss,devLoss,trainAccr,devAccr):\n",
        "\n",
        "  fig,ax = plt.subplots(1,2,figsize=(16,5))\n",
        "\n",
        "  ax[0].plot(trainLoss,'s-',label='Train')\n",
        "  ax[0].plot(devLoss,'o-',label='Dev')\n",
        "  ax[0].set_xlabel('Epochs')\n",
        "  ax[0].set_ylabel('Loss (Cross Entropy)')\n",
        "  ax[0].set_title('Model loss')\n",
        "\n",
        "  ax[1].plot(trainAccr,'s-',label='Train')\n",
        "  ax[1].plot(devAccr,'o-',label='Dev')\n",
        "  ax[1].set_xlabel('Epochs')\n",
        "  ax[1].set_ylabel('Accuracy rates (%)')\n",
        "  ax[1].set_title(f'Final model test accuracy rate: {devAccr[-1]:.2f}%')\n",
        "  ax[1].legend()\n",
        "\n",
        "  plt.show()\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oTrOm96BsfA"
      },
      "source": [
        "# ~2 minutes with 5 epochs on GPU\n",
        "trainLoss,devLoss,trainAccr,devAccr,net = function2trainTheModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG3P2YkUDwZK"
      },
      "source": [
        "### ----  Convert RGB to Grayscale ---- ###\n",
        "\n",
        "#trainTransform  = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
        " #                                   transforms.ToTensor(), \n",
        "  #                                  transforms.Normalize((0.5), (0.5))])\n",
        "#trainset = datasets.CIFAR10(root='./data', train=True, download=True,\n",
        " #   transform=trainTransform)\n",
        " #trainset[0][0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RKoLCdGD-a-"
      },
      "source": [
        "# next issue: do we need to normalize the images?\n",
        "\n",
        "#plt.hist(images[:10,:,:,:].view(1,-1).detach(),40);\n",
        "#plt.title('Raw values')\n",
        "#plt.show()\n",
        "\n",
        "# yarp.\n",
        "#images /= torch.max(images)\n",
        "\n",
        "#plt.hist(images[:10,:,:,:].view(1,-1).detach(),40);\n",
        "#plt.title('After normalization')\n",
        "#plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnCOwC9BEAqc"
      },
      "source": [
        "# transformations\n",
        "#transform = T.Compose([ T.ToTensor(),\n",
        " #                       T.Normalize([.5,.5,.5],[.5,.5,.5])\n",
        "  #                     ])\n",
        "\n",
        "# import the data and simultaneously apply the transform\n",
        "#trainset = torchvision.datasets.CIFAR10(root='./data', train=True,  download=True, transform=transform)\n",
        "#devtest  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# split the devtest into two separate sets\n",
        "#randidx = np.random.permutation(10000)   # a random permutation of indices\n",
        "#devset  = Subset(devtest,randidx[:6000]) # Subset for devset\n",
        "#testset = Subset(devtest,randidx[6000:]) # Subset for test\n",
        "\n",
        "# transform to dataloaders\n",
        "#batchsize    = 32\n",
        "#train_loader = DataLoader(trainset,batch_size=batchsize,shuffle=True,drop_last=True)\n",
        "#dev_loader   = DataLoader(devset,  batch_size=batchsize) # note: devtest in batches!\n",
        "#test_loader  = DataLoader(testset, batch_size=len(testset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EteMISj_ECkD"
      },
      "source": [
        "# check size (should be images X channels X width X height\n",
        "#print( train_loader.dataset.tensors[0].shape )\n",
        "#print( train_loader.dataset.tensors[1].shape )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CcP00jFEEvj"
      },
      "source": [
        "# Step 1: use scikitlearn to split the data dev and train sets\n",
        "#train_data, dev_data, train_labels, dev_labels = train_test_split(images, labels, test_size=.1)\n",
        "\n",
        "# Step 2: use scikitlearn to split the data dev and tes sets\n",
        "#dev_data, test_data, dev_labels, test_labels = train_test_split(dev_data, dev_labels, test_size=.5)\n",
        "\n",
        "# print(type(train_labels))\n",
        "# print(type(dev_labels))\n",
        "# print(type(test_labels))\n",
        "# print(torch.FloatTensor(train_labels))\n",
        "\n",
        "# Step 3: convert into PyTorch Datasets\n",
        "#train_data = TensorDataset(train_data, torch.Tensor(train_labels))\n",
        "#dev_data = TensorDataset(dev_data, torch.Tensor(dev_labels))\n",
        "#test_data  = TensorDataset(test_data, torch.Tensor(test_labels))\n",
        "\n",
        "# Step 4: translate into dataloader objects\n",
        "#batchsize    = 32\n",
        "#train_loader = DataLoader(train_data,batch_size=batchsize,shuffle=True,drop_last=True)\n",
        "#dev_loader  = DataLoader(dev_data,batch_size=dev_data.tensors[0].shape[0])\n",
        "#test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLXaBQd5EGrj"
      },
      "source": [
        "# inspect a few random images\n",
        "\n",
        "#fig,axs = plt.subplots(5,5,figsize=(10,10))\n",
        "\n",
        "#for ax in axs.flatten():\n",
        "\n",
        "  # select a random picture\n",
        "  #randidx = np.random.choice(len(cdata.targets))\n",
        "\n",
        "  # extract that image\n",
        "  # using numpy array not tensor\n",
        "  #pic = cdata.data[randidx,:,:,:]\n",
        "  # and its label\n",
        "  #label = cdata.classes[cdata.targets[randidx]]\n",
        "\n",
        "  # and show!\n",
        "  #ax.imshow(pic)\n",
        "  #ax.text(16,0,label,ha='center',fontweight='bold',color='k',backgroundcolor='y')\n",
        "  #ax.axis('off')\n",
        "\n",
        "#plt.tight_layout()\n",
        "#plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Hf56A4NEIdN"
      },
      "source": [
        "#classes = cdata.classes\n",
        "#print(classes)\n",
        "#labels = cdata.targets\n",
        "#print(type(labels))\n",
        "#images = torch.from_numpy(cdata.data).float()\n",
        "#images = images.reshape(50000, 3, 32, 32)\n",
        "#print(images.shape)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}